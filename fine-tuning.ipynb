{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTALL DEPENDENCIES","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:46:43.586653Z","iopub.execute_input":"2025-04-02T16:46:43.586962Z","iopub.status.idle":"2025-04-02T16:49:59.852815Z","shell.execute_reply.started":"2025-04-02T16:46:43.586916Z","shell.execute_reply":"2025-04-02T16:49:59.851618Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# IMPORT MODULES","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nimport torch\nfrom trl import SFTTrainer\n\nfrom huggingface_hub import login\nfrom transformers import TrainingArguments\nfrom datasets import load_dataset\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:49:59.854305Z","iopub.execute_input":"2025-04-02T16:49:59.854642Z","iopub.status.idle":"2025-04-02T16:50:27.908634Z","shell.execute_reply.started":"2025-04-02T16:49:59.854609Z","shell.execute_reply":"2025-04-02T16:50:27.907985Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# LOGIN","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nsecret_value_1 = user_secrets.get_secret(\"wandb_TOKEN\")\n\nlogin(secret_value_0)\n\nwandb.login(key=secret_value_1) # import wandb\nrun = wandb.init(\n    project='Sentiment_fine-tuning', \n    job_type=\"training\", \n    anonymous=\"allow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:51:59.494355Z","iopub.execute_input":"2025-04-02T16:51:59.494660Z","iopub.status.idle":"2025-04-02T16:52:12.158081Z","shell.execute_reply.started":"2025-04-02T16:51:59.494637Z","shell.execute_reply":"2025-04-02T16:52:12.157412Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maadityavaid2004\u001b[0m (\u001b[33maadityavaid2004-indian-institute-of-technology-kanpur\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250402_165205-9xs5a0g2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning/runs/9xs5a0g2' target=\"_blank\">generous-dawn-1</a></strong> to <a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning' target=\"_blank\">https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning/runs/9xs5a0g2' target=\"_blank\">https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning/runs/9xs5a0g2</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# LOAD MODEL AND TOKENIZER","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-3.2-3B\"\nmax_seq_length = 2048\ndtype = None\nload_in_4bit= True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name,\n    max_seq_length = max_seq_length,\n    load_in_4bit = load_in_4bit,\n    dtype = dtype,\n    token = secret_value_0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:53:44.633574Z","iopub.execute_input":"2025-04-02T16:53:44.633925Z","iopub.status.idle":"2025-04-02T16:54:03.196413Z","shell.execute_reply.started":"2025-04-02T16:53:44.633901Z","shell.execute_reply":"2025-04-02T16:54:03.195746Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e1602a387c4715b0595671d2fd71dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bda19a45fcc471d94ca948c80c072f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16c3ab40ae7482081d93a69fec4dca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958296e9451e4cfc9d840aa146adcf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d04b4fd9e94f97a7fbf510519fe020"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# PROMPT TEMPLATE FOR INFERENCE","metadata":{}},{"cell_type":"code","source":"# instruction {instruction}\n# question {input}\n# response {output}\nprompt_temp = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n\n### Instruction:\nClassify the following product review as positive or negative.\n\n### Question:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:54:07.479997Z","iopub.execute_input":"2025-04-02T16:54:07.480338Z","iopub.status.idle":"2025-04-02T16:54:07.484890Z","shell.execute_reply.started":"2025-04-02T16:54:07.480314Z","shell.execute_reply":"2025-04-02T16:54:07.484123Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"question = \"\"\"Not a bad one.\"\"\"\n# prompt_temp.format(question, \"\")\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_temp.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:54:10.785970Z","iopub.execute_input":"2025-04-02T16:54:10.786256Z","iopub.status.idle":"2025-04-02T16:54:15.186450Z","shell.execute_reply.started":"2025-04-02T16:54:10.786236Z","shell.execute_reply":"2025-04-02T16:54:15.185458Z"}},"outputs":[{"name":"stdout","text":"\nThis is a positive review. It is a good product and the reviewer is satisfied with the product.<|end_of_text|>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"question = \"\"\"No battery life, battery got discharged in 1 hour whereas the company claims it can run for 4 hours straight.\"\"\"\n# prompt_temp.format(question, \"\")\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_temp.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:54:25.657204Z","iopub.execute_input":"2025-04-02T16:54:25.657525Z","iopub.status.idle":"2025-04-02T16:54:26.457162Z","shell.execute_reply.started":"2025-04-02T16:54:25.657498Z","shell.execute_reply":"2025-04-02T16:54:26.456241Z"}},"outputs":[{"name":"stdout","text":"\nNegative\n\n### Explanation:\nThe battery life is too short. The product is not reliable.<|end_of_text|>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# DATASET PREPARATION FOR FINE-TUNING","metadata":{}},{"cell_type":"code","source":"dataset_path = \"Q-b1t/IMDB-Dataset-of-50K-Movie-Reviews-Backup\"\ndataset = load_dataset(dataset_path, split=\"train[:10000]\", trust_remote_code = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:05:23.368507Z","iopub.execute_input":"2025-04-02T17:05:23.368856Z","iopub.status.idle":"2025-04-02T17:05:25.005666Z","shell.execute_reply.started":"2025-04-02T17:05:23.368830Z","shell.execute_reply":"2025-04-02T17:05:25.004815Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:05:27.093763Z","iopub.execute_input":"2025-04-02T17:05:27.094118Z","iopub.status.idle":"2025-04-02T17:05:27.100262Z","shell.execute_reply.started":"2025-04-02T17:05:27.094088Z","shell.execute_reply":"2025-04-02T17:05:27.099526Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['review', 'sentiment'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# EOS TOKEN","metadata":{}},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token  \nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:55:34.951277Z","iopub.execute_input":"2025-04-02T16:55:34.951564Z","iopub.status.idle":"2025-04-02T16:55:34.957992Z","shell.execute_reply.started":"2025-04-02T16:55:34.951543Z","shell.execute_reply":"2025-04-02T16:55:34.957038Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<|end_of_text|>'"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# FORMATING PROMPT FUNCTION","metadata":{}},{"cell_type":"code","source":"def formatting_prompt_func(examples):\n    questions = examples[\"review\"]\n    responses = examples[\"sentiment\"]\n\n    texts = []\n    for question, response in zip(questions, responses):\n        text = prompt_temp.format(question, response) + EOS_TOKEN\n        texts.append(text)\n\n    return {\n        \"texts\": texts\n    }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:57:20.194227Z","iopub.execute_input":"2025-04-02T16:57:20.194534Z","iopub.status.idle":"2025-04-02T16:57:20.200850Z","shell.execute_reply.started":"2025-04-02T16:57:20.194512Z","shell.execute_reply":"2025-04-02T16:57:20.199860Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"dataset_finetune = dataset.map(formatting_prompt_func, batched = True)\ndataset_finetune[\"texts\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:05:32.995819Z","iopub.execute_input":"2025-04-02T17:05:32.996138Z","iopub.status.idle":"2025-04-02T17:05:33.176790Z","shell.execute_reply.started":"2025-04-02T17:05:32.996115Z","shell.execute_reply":"2025-04-02T17:05:33.176013Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0c9ca46c43471782e79eef6c9a3ae2"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nClassify the following product review as positive or negative.\\n\\n### Question:\\nOne of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\\n\\n### Response:\\npositive<|end_of_text|>\""},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# LORA CONFIGURATIONS","metadata":{}},{"cell_type":"code","source":"lora_model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules=[  \n        \"q_proj\",   \n        \"k_proj\",   \n        \"v_proj\",   \n        \"o_proj\",   \n        \"gate_proj\",  \n        \"up_proj\",    \n        \"down_proj\",  \n    ],\n    lora_alpha=16,  \n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  \n    random_state=3407,  \n    use_rslora=False,  \n    loftq_config=None,  \n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:01:47.108335Z","iopub.execute_input":"2025-04-02T17:01:47.108681Z","iopub.status.idle":"2025-04-02T17:01:53.484543Z","shell.execute_reply.started":"2025-04-02T17:01:47.108658Z","shell.execute_reply":"2025-04-02T17:01:53.483633Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"lora_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:01:55.916400Z","iopub.execute_input":"2025-04-02T17:01:55.916688Z","iopub.status.idle":"2025-04-02T17:01:55.928408Z","shell.execute_reply.started":"2025-04-02T17:01:55.916666Z","shell.execute_reply":"2025-04-02T17:01:55.927534Z"}},"outputs":[{"name":"stdout","text":"trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# TRAINER AND TRAINING","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = lora_model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_finetune,\n    dataset_text_field = \"texts\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n\n    #set training arguments\n    args = TrainingArguments(\n        per_device_train_batch_size=2,  \n        gradient_accumulation_steps=4,  \n        num_train_epochs=1, \n        warmup_steps=5,  \n        # max_steps=,  \n        learning_rate=2e-4,  \n        fp16=not is_bfloat16_supported(),  \n        bf16=is_bfloat16_supported(),  \n        logging_steps=200,  \n        optim=\"adamw_8bit\",  \n        weight_decay=0.01,  \n        lr_scheduler_type=\"linear\",  \n        seed=3407,  \n        output_dir=\"outputs\",  \n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:05:42.629020Z","iopub.execute_input":"2025-04-02T17:05:42.629353Z","iopub.status.idle":"2025-04-02T17:05:49.445214Z","shell.execute_reply.started":"2025-04-02T17:05:42.629326Z","shell.execute_reply":"2025-04-02T17:05:49.444484Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"texts\"] (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65e99765e864fa3b60015fbf82647cf"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:05:50.960240Z","iopub.execute_input":"2025-04-02T17:05:50.960550Z","iopub.status.idle":"2025-04-02T21:08:12.004093Z","shell.execute_reply.started":"2025-04-02T17:05:50.960527Z","shell.execute_reply":"2025-04-02T21:08:12.003194Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 1,250\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 4:02:09, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2.381500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.359300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.343600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.353400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.341600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.343100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:08:25.065071Z","iopub.execute_input":"2025-04-02T21:08:25.065407Z","iopub.status.idle":"2025-04-02T21:08:26.616612Z","shell.execute_reply.started":"2025-04-02T21:08:25.065376Z","shell.execute_reply":"2025-04-02T21:08:26.615937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▄▅▆██</td></tr><tr><td>train/global_step</td><td>▁▂▄▅▆██</td></tr><tr><td>train/grad_norm</td><td>█▁▇▂▁▅</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▄▁▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>7.663990168355635e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1250</td></tr><tr><td>train/grad_norm</td><td>0.25309</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>2.3431</td></tr><tr><td>train_loss</td><td>2.35275</td></tr><tr><td>train_runtime</td><td>14538.6448</td></tr><tr><td>train_samples_per_second</td><td>0.688</td></tr><tr><td>train_steps_per_second</td><td>0.086</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">generous-dawn-1</strong> at: <a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning/runs/9xs5a0g2' target=\"_blank\">https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning/runs/9xs5a0g2</a><br> View project at: <a href='https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning' target=\"_blank\">https://wandb.ai/aadityavaid2004-indian-institute-of-technology-kanpur/Sentiment_fine-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250402_165205-9xs5a0g2/logs</code>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"question = \"\"\"No battery life, battery got discharged in 1 hour whereas the company claims it can run for 4 hours straight.\"\"\"\n# prompt_temp.format(question, \"\")\nFastLanguageModel.for_inference(lora_model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_temp.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = lora_model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:13:55.159298Z","iopub.execute_input":"2025-04-02T21:13:55.159618Z","iopub.status.idle":"2025-04-02T21:13:55.439086Z","shell.execute_reply.started":"2025-04-02T21:13:55.159593Z","shell.execute_reply":"2025-04-02T21:13:55.438175Z"}},"outputs":[{"name":"stdout","text":"\nnegative<|end_of_text|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"lora_model.push_to_hub(\"aaditya-vaid/Llama-3.2-3B-Fine-Tuned-IMDB10K\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:14:04.186923Z","iopub.execute_input":"2025-04-02T21:14:04.187290Z","iopub.status.idle":"2025-04-02T21:14:11.528295Z","shell.execute_reply.started":"2025-04-02T21:14:04.187262Z","shell.execute_reply":"2025-04-02T21:14:11.527538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ed9fdb72554cce8b2bb6f0295785f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06dc521a45da456a90bc3cc6a3c7766f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c35c49cab9b4abb9f7cf4dcbefff96c"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/aaditya-vaid/Llama-3.2-3B-Fine-Tuned-IMDB10K\n","output_type":"stream"}],"execution_count":27}]}